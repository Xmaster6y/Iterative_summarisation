{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4-VUn1NllcI"
      },
      "source": [
        "# Iterative summarization interpretability notebook\n",
        "\n",
        "## Goal\n",
        "\n",
        "* Interpret a couple of summarizer/exemplifier networks.\n",
        "\n",
        "* Follow the stream of information during a discussion between the two networks.\n",
        "\n",
        "## Notes\n",
        "\n",
        "* The bellow code is inspired (sometimes copied) from Neel Nanda's [demo notebook](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/Main_Demo.ipynb) and his library [TransformerLens](https://github.com/neelnanda-io/TransformerLens)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzRZwHNXp9Xi"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqqGMo6qiQNQ"
      },
      "source": [
        "### Pip installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEZwj6TYiNwv"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "!pip install circuitsvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0QZjGnOSju_"
      },
      "source": [
        "### Classic libraries imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-_bc5gGS0rR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APwXAuWMS32u"
      },
      "source": [
        "### External toolboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4CnamlFqloW"
      },
      "outputs": [],
      "source": [
        "import circuitsvis as cv\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens import HookedTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDMsi1EPp_St"
      },
      "source": [
        "## Model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCBkgZv0qCny"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.empty_cache()\n",
        "sum_model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
        "exp_model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Q8asI5aVHN"
      },
      "outputs": [],
      "source": [
        "sum_weight_file = './sum_weights.pt'\n",
        "exp_weight_file = './exp_weights.pt'\n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILEID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1tPU5mHCXcAxZJJHvv9XyT-MzgoeSWUk9\" -O $sum_weight_file  && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILEID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1--qi-Rzhff4OcAtknrSrzfDzgNC9z5mQ\" -O $exp_weight_file  && rm -rf /tmp/cookies.txt\n",
        "\n",
        "if not os.path.exists(sum_weight_file):\n",
        "    raise FileNotFoundError\n",
        "else:\n",
        "    sum_model.load_state_dict(torch.load(sum_weight_file))\n",
        "    sum_model.eval()\n",
        "if not os.path.exists(exp_weight_file):\n",
        "    raise FileNotFoundError\n",
        "else:\n",
        "    exp_model.load_state_dict(torch.load(exp_weight_file))\n",
        "    exp_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SEyiZG7u0jM"
      },
      "source": [
        "## Iterative summarization test\n",
        "\n",
        "*  The main idea is to plug to complementary networks to create a discussion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAVIInKNuz7p"
      },
      "outputs": [],
      "source": [
        "def discuss(exp_model, sum_model, starting_review, discussion_length=5, sum_max_new_tokens=20, exp_max_new_tokens=200):\n",
        "    to_pred = f\"[review]: {starting_review}\\n[summary]: \"\n",
        "    discussion = to_pred\n",
        "    for _ in range(discussion_length):\n",
        "        sum_out = sum_model.generate(to_pred, max_new_tokens=sum_max_new_tokens)\n",
        "        sum_gen = sum_out.split('[summary]: ')[1]\n",
        "        sum_gen = sum_gen.split('<|endoftext|>')[0]\n",
        "        print(sum_gen)\n",
        "        to_pred = f\"[summary]: {sum_gen}\\n[review]: \"\n",
        "        discussion += sum_gen\n",
        "        exp_out = exp_model.generate(to_pred, max_new_tokens=exp_max_new_tokens)\n",
        "        exp_gen = exp_out.split('[review]: ')[1]\n",
        "        exp_gen = exp_gen.split('<|endoftext|>')[0]\n",
        "        print(exp_gen)\n",
        "        to_pred = f\"[review]: {exp_gen}\\n[summary]: \"\n",
        "        discussion += \"\\n\" + to_pred\n",
        "    return discussion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-iyhiMCySFU"
      },
      "outputs": [],
      "source": [
        "train_review = \"I love this adaptation of the classic tale.  Henry Winkler is, of course, one of my favorite actors.  This is a different slant from the original but it gets the message across none the less.  Well worth the viewing.\"\n",
        "train_summary = \"Good Adaptation of the Classic Tale\"\n",
        "eval_review = \"The counter scene when Allen's character says he is robbing the bank and has a \\\"gub\\\".  That is hilarious!! Many more humorous scenes!\\nOne of America's best comics ever!!\"\n",
        "eval_summary = \"Very Funny\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLFfxJz2xbxU"
      },
      "outputs": [],
      "source": [
        "discussion_length = 3\n",
        "train_discussion = discuss(exp_model, sum_model, train_review, discussion_length=discussion_length)\n",
        "print(train_discussion)\n",
        "eval_discussion = discuss(exp_model, sum_model, eval_review, discussion_length=discussion_length)\n",
        "print(eval_discussion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gthg8ax509r"
      },
      "source": [
        "## Plot helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur3_RZrn9SAX"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk2JTjrH53d6"
      },
      "outputs": [],
      "source": [
        "def attention_patterns(model, text, layer=0):\n",
        "    print(f\"Text:\\n{text}\")\n",
        "    tokens = sum_model.to_tokens(gen_text)\n",
        "    logits, cache = sum_model.run_with_cache(tokens, remove_batch_dim=True)\n",
        "    attention_pattern = cache[\"pattern\", layer, \"attn\"]\n",
        "    str_tokens = sum_model.to_str_tokens(text)\n",
        "    return cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGBZg_fvqxsE"
      },
      "source": [
        "## Summarizer plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjguGPDPSfIA"
      },
      "outputs": [],
      "source": [
        "gen_text = sum_model.generate(f\"[review]: {train_review}\\n[summary]: \", max_new_tokens=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdITQ0MmZPrj"
      },
      "source": [
        "### Attention patterns for generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFMiPII6Yc4K"
      },
      "outputs": [],
      "source": [
        "layer = 0\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(sum_model, gen_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of-lH3F1Ym1W"
      },
      "outputs": [],
      "source": [
        "layer = 5\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(sum_model, gen_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tda67W7Z-U6L"
      },
      "outputs": [],
      "source": [
        "layer = 10\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(sum_model, gen_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NTBpuDtYtnr"
      },
      "outputs": [],
      "source": [
        "train_text = f\"[review]: {train_review}\\n[summary]: {train_summary}\"\n",
        "eval_text = f\"[review]: {eval_review}\\n[summary]: {eval_summary}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69CdRpK5ZhKX"
      },
      "source": [
        "### Attention patterns for an eval sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH3LgVTrZLUl"
      },
      "outputs": [],
      "source": [
        "layer = 0\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(sum_model, eval_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3iuw8r2ZLUn"
      },
      "outputs": [],
      "source": [
        "layer = 5\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(sum_model, eval_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcE8-CtxZLUp"
      },
      "outputs": [],
      "source": [
        "layer = 11\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(sum_model, eval_text, layer=layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAFAhHrHZlO5"
      },
      "source": [
        "### Copying score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcsHkLmD8tLq"
      },
      "outputs": [],
      "source": [
        "OV_circuit_all_heads = sum_model.OV\n",
        "OV_circuit_all_heads_eigenvalues = OV_circuit_all_heads.eigenvalues \n",
        "OV_copying_score = OV_circuit_all_heads_eigenvalues.sum(dim=-1).real / OV_circuit_all_heads_eigenvalues.abs().sum(dim=-1)\n",
        "imshow(utils.to_numpy(OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head\", zmax=1.0, zmin=-1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNjztRAxZwQZ"
      },
      "source": [
        "## Exemplifier plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Tm-Ff8ZwQb"
      },
      "outputs": [],
      "source": [
        "gen_text = exp_model.generate(f\"[summary]: {train_summary}\\n[review]: \", max_new_tokens=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykRPkaSyZwQc"
      },
      "source": [
        "### Attention patterns for generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcmyg47oZwQc"
      },
      "outputs": [],
      "source": [
        "layer = 0\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(exp_model, gen_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjp41UxnZwQd"
      },
      "outputs": [],
      "source": [
        "layer = 5\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(exp_model, gen_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jowtP9mRZwQe"
      },
      "outputs": [],
      "source": [
        "layer = 10\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(exp_model, gen_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfePh3sTZwQf"
      },
      "outputs": [],
      "source": [
        "train_text = f\"[summary]: {train_summary}\\n[review]: {train_review}\"\n",
        "eval_text = f\"[summary]: {eval_summary}\\n[review]: {eval_review}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGk9n84pZwQi"
      },
      "source": [
        "### Attention patterns for an eval sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOFfTWPPZwQi"
      },
      "outputs": [],
      "source": [
        "layer = 0\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(exp_model, eval_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JApQEOlZZwQi"
      },
      "outputs": [],
      "source": [
        "layer = 5\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(exp_model, eval_text, layer=layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88kgCBrUZwQj"
      },
      "outputs": [],
      "source": [
        "layer = 11\n",
        "print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "attention_patterns(exp_model, eval_text, layer=layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAiT2-bbZwQj"
      },
      "source": [
        "### Copying score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYUJdolsZwQk"
      },
      "outputs": [],
      "source": [
        "OV_circuit_all_heads = exp_model.OV\n",
        "OV_circuit_all_heads_eigenvalues = OV_circuit_all_heads.eigenvalues \n",
        "OV_copying_score = OV_circuit_all_heads_eigenvalues.sum(dim=-1).real / OV_circuit_all_heads_eigenvalues.abs().sum(dim=-1)\n",
        "imshow(utils.to_numpy(OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head\", zmax=1.0, zmin=-1.0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}